{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1cbd979-9348-4147-b9b3-4f03081026de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import open_clip\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import diffusers \n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from huggingface_hub import hf_hub_download\n",
    "from models import CLIPVisionTower\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6565d34a-7705-454a-8531-a7fbf5d42136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./models.py'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_hub_download(repo_id=\"AIRI-Institute/OmniFusion\", filename=\"OmniMistral-v1_1/projection.pt\", local_dir='./')\n",
    "hf_hub_download(repo_id=\"AIRI-Institute/OmniFusion\", filename=\"models.py\", local_dir='./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d93c70f5-093a-4612-a5a1-fb66db63f027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 12 20:52:19 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  | 00000000:4B:00.0 Off |                    0 |\n",
      "| N/A   31C    P0              68W / 400W |      4MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a6e3d3b-77c5-49f9-b0b4-c11fa335275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acd197d1-824a-4a0e-921b-56f4ae4daad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_anygpt = load_dataset(\"zhanjun/AnyGPT-data-image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "068f5145-9ef8-49bd-bb55-9d2abda24606",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_science = load_dataset(\"cnut1648/ScienceQA-LLAVA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00f5341f-87bc-4277-b991-8de8da8b2978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'validation-0',\n",
       " 'image': None,\n",
       " 'conversations': [{'from': 'human',\n",
       "   'value': \"Context: N/A\\nQuestion: What does the verbal irony in this text suggest?\\nAccording to Mr. Herrera's kids, his snoring is as quiet as a jackhammer.\\nOptions: (A) The snoring is loud. (B) The snoring occurs in bursts.\"},\n",
       "  {'from': 'gpt', 'value': 'The answer is A.'}],\n",
       " 'question': \"What does the verbal irony in this text suggest?\\nAccording to Mr. Herrera's kids, his snoring is as quiet as a jackhammer.\",\n",
       " 'context': 'N/A',\n",
       " 'choice': '(A) The snoring is loud. (B) The snoring occurs in bursts.',\n",
       " 'answer': 'A',\n",
       " 'lecture': 'Figures of speech are words or phrases that use language in a nonliteral or unusual way. They can make writing more expressive.\\nVerbal irony involves saying one thing but implying something very different. People often use verbal irony when they are being sarcastic.\\nOlivia seems thrilled that her car keeps breaking down.\\nEach breakdown is as enjoyable as a punch to the face.',\n",
       " 'solution': \"The text uses verbal irony, which involves saying one thing but implying something very different.\\nAs quiet as a jackhammer suggests that the snoring is loud. A jackhammer is not quiet, and neither is Mr. Herrera's snoring.\"}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_science['validation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ca56f82-c523-430e-bab9-7935a083225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip, preprocess_train, preprocess_val = open_clip.create_model_and_transforms('hf-hub:laion/CLIP-ViT-bigG-14-laion2B-39B-b160k')\n",
    "clip.to(DEVICE);\n",
    "clip.visual.output_tokens = True\n",
    "\n",
    "def preprocess_image(image_path, preprocess_function=preprocess_train):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    return preprocess_function(image)\n",
    "\n",
    "def encode_image(path=None, image=None, preprocess_function=preprocess_train):\n",
    "    if path:\n",
    "        image_tensor = preprocess_image(path, preprocess_function).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            hidden_states = clip.visual(image_tensor[None])[1].to(DEVICE).squeeze(0)\n",
    "        return hidden_states\n",
    "    \n",
    "    if image:\n",
    "        image_tensor = preprocess_function(image).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            hidden_states = clip.visual(image_tensor[None])[1].to(DEVICE).squeeze(0)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80e801eb-1a91-427f-9f9c-6fc52bfe2c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aa3e31b1d29443daeceee1371e0762c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97a1bf5e09874cf4b3a036a08748816d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"AIRI-Institute/OmniFusion\", subfolder=\"OmniMistral-v1_1/tuned-model\", torch_dtype=torch.bfloat16, device_map=DEVICE)\n",
    "\n",
    "projection = torch.load(\"OmniMistral-v1_1/projection.pt\", map_location=DEVICE)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"AIRI-Institute/OmniFusion\", subfolder=\"OmniMistral-v1_1/tokenizer\", use_fast=False)\n",
    "\n",
    "clip = CLIPVisionTower(\"openai/clip-vit-large-patch14-336\")\n",
    "clip.load_model()\n",
    "clip = clip.to(device=DEVICE, dtype=torch.bfloat16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46b657ec-1532-48b8-9ece-80c1789352b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "choice='validation'\n",
    "i  = random.randint(0, len(dataset_science[choice])-1)\n",
    "image = None\n",
    "while not image:\n",
    "    i  = random.randint(0, len(dataset_science[choice])-1)\n",
    "    item = dataset_science[choice][i]\n",
    "    image = item['image']\n",
    "conversation = item['conversations']\n",
    "if type(conversation) == str:\n",
    "    conversation = ast.literal_eval(conversation)\n",
    "query = conversation[0]['value'].replace('\\n<image>', '').replace('\\n', ' ')\n",
    "answer = item['solution'].replace('\\n', ' ').strip() + '</s>'\n",
    "with torch.no_grad():\n",
    "    image_features = clip.image_processor(image, return_tensors='pt')\n",
    "    image_embedding = clip(image_features['pixel_values']).to(device=DEVICE, dtype=torch.bfloat16)\n",
    "\n",
    "    caption_ids = list(tokenizer.encode(answer, add_special_tokens=False))\n",
    "    caption_ids = torch.LongTensor(caption_ids).to(DEVICE)\n",
    "    user_query_ids = tokenizer.encode(query, add_special_tokens=False, return_tensors=\"pt\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9559e3d3-130a-47ae-959f-388f6b25cbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_embs = image_embedding\n",
    "projected_vision_embs = projection(vision_embs)\n",
    "projected_vision_embs = projected_vision_embs\n",
    "text_embeddings = model.model.embed_tokens(caption_ids)\n",
    "prompt_embeddings = model.model.embed_tokens(user_query_ids)\n",
    "bs = projected_vision_embs.shape[0]\n",
    "\n",
    "\n",
    "embeddings1 = torch.cat([\n",
    "            projected_vision_embs,\n",
    "            prompt_embeddings\n",
    "            ],\n",
    "            dim=1)\n",
    "\n",
    "embeddings2 = text_embeddings.repeat(bs,1,1)\n",
    "embeddings = torch.cat([embeddings1, embeddings2], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39a3fae1-d3ff-41f1-8970-b2ba2b70f737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([111]), torch.Size([1, 119]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption_ids.shape, user_query_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc66f19f-6b27-4503-a8ec-b2d6a4c315d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([111, 4096]),\n",
       " torch.Size([1, 119, 4096]),\n",
       " torch.Size([1, 119, 4096]),\n",
       " torch.Size([1, 576, 4096]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embeddings.shape, prompt_embeddings.shape,prompt_embeddings.repeat(bs,1,1).shape, projected_vision_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0288e0ae-e09d-4990-a293-5ce893711366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 695, 4096]),\n",
       " torch.Size([1, 111, 4096]),\n",
       " torch.Size([1, 111, 4096]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings1.shape, embeddings2.shape, embeddings2.repeat(bs,1,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bba9ab06-2b20-4a5a-b679-0ab4c0076140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bad_words_ids = tokenizer([\"\\n\", \"</s>\", \":\"], add_special_tokens=False).input_ids + [[13]]\n",
    "# gen_params = {\n",
    "#         \"do_sample\": False,\n",
    "#         \"max_new_tokens\": 50,\n",
    "#         \"early_stopping\": True,\n",
    "#         \"num_beams\": 3,\n",
    "#         \"repetition_penalty\": 1.0,\n",
    "#         \"remove_invalid_values\": True,\n",
    "#         \"eos_token_id\": 2,\n",
    "#         \"pad_token_id\": 2,\n",
    "#         \"forced_eos_token_id\": 2,\n",
    "#         \"use_cache\": True,\n",
    "#         \"no_repeat_ngram_size\": 4,\n",
    "#         \"bad_words_ids\": bad_words_ids,\n",
    "#         \"num_return_sequences\": 1,\n",
    "#     }\n",
    "# out = model.generate(inputs_embeds=embeddings, **gen_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "396eb5c6-95d0-4496-a5ce-16d894919f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 9]), torch.Size([1, 8]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape, out[:, 1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07b992d3-2d08-40a4-bf01-ab8e56ec5313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 806, 4096])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[None, ...].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2451058e-70d0-4ab5-94bf-f2230c0db35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAELoss(nn.Module):\n",
    "    def __init__(self, 位=1.):\n",
    "        super().__init__()\n",
    "        self.位 = 位\n",
    "        self.reconstruction_loss = nn.BCELoss()\n",
    "    \n",
    "    def forward(self, output, target, vq_loss):\n",
    "        reconst_loss = self.reconstruction_loss(output, target)\n",
    "        \n",
    "        loss = reconst_loss + self.位 * vq_loss\n",
    "        return {\"loss\": loss, \"reconstruction loss\": reconst_loss, \"VQ loss\": vq_loss}\n",
    "loss_vae = VAELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b77e8da0-59e7-4a82-a911-a9060720e2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ae = diffusers.VQModel(1, 1).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9421d3a1-ab41-4d7d-b19d-dda452b71c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = model_ae(embeddings[None, ...].float())\n",
    "h = model_ae.encode(embeddings[None, ...].float()).latents\n",
    "_, vq_loss, _ = model_ae.quantize(h)\n",
    "# out = model_ae.decode(h).sample\n",
    "# loss = loss_vae(out, embeddings[None, ...].float(), vq_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebe415ec-918f-40fb-b3a1-c815791ccdff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1261, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vq_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6ff0a3a-ea87-4ee4-9c4d-7fdd178e6512",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model_ae.decode(h).sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70945caa-d4d5-4498-9f32-3bd9e48a6d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 2.2227,  1.3053,  0.8109,  ..., -1.0408, -1.2240,  1.0533],\n",
       "          [ 0.1085, -0.2438, -1.9387,  ..., -0.5151, -4.7525,  1.7800],\n",
       "          [-2.6457,  2.7697, -3.5529,  ...,  0.6989,  1.3130,  2.7379],\n",
       "          ...,\n",
       "          [-3.6787,  2.6983,  1.2140,  ...,  0.2783, -3.1924, -2.7261],\n",
       "          [-2.7598,  1.8086,  4.5208,  ...,  0.2744, -1.6552,  1.8046],\n",
       "          [-1.5706,  3.2711,  2.4797,  ...,  1.2999,  1.6929,  1.0233]]]],\n",
       "       device='cuda:0', grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7cbb1870-4dc8-4cfa-9c7d-185d6d5cd1f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5).softmax(dim=1)\n",
    "output = loss(input, target)\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a69e833-4071-41d1-99de-536fbbd8539b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_loss = nn.CrossEntropyLoss()\n",
    "reconst_loss = reconstruction_loss(out, embeddings[None, ...].float())\n",
    "loss = reconst_loss + vq_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c8ef514-9c11-401d-b6c6-b650c2847b31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 743, 4096])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6afd8158-a6e0-4128-94f4-22edf7895107",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Sigmoid()\n",
    "loss = nn.BCELoss()\n",
    "input = torch.randn((1,1,743,4096), requires_grad=True)\n",
    "target = torch.rand((1,1,743,4096), requires_grad=False)\n",
    "output = loss(m(input), target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3679d889-657a-44e0-988b-e9fb3ed33d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 2]), torch.Size([3, 2]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(input).shape, target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3b269c1d-3833-409d-b2c0-09d83ac98abe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8058, grad_fn=<BinaryCrossEntropyBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76efbe4f-feab-46e3-9528-c4f818d81614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1261, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719cf48e-cc1c-4d50-a3cf-2eb2e4cc9586",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
